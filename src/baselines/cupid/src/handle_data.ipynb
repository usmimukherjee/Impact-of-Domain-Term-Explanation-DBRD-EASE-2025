{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| run: 1\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n",
      "ic| mixed_keywords: 248\n",
      "ic| run: 2\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n",
      "ic| mixed_keywords: 247\n",
      "ic| run: 3\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n",
      "ic| mixed_keywords: 249\n",
      "ic| run: 4\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n",
      "ic| mixed_keywords: 254\n",
      "ic| run: 5\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n",
      "ic| mixed_keywords: 241\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "import re, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_generated_by_gpt(project, original_csv, to_save_csv, run, gpt_variant, flag_file):\n",
    "    original_df = pd.read_csv(original_csv)\n",
    "    bug_ids = []\n",
    "    summaries, descriptions = [], []\n",
    "    all_bugs = set()\n",
    "    found_bugs = set()\n",
    "    modified_bug_count = 0\n",
    "    mixed_keywords = 0\n",
    "    flag_df = pd.read_csv(flag_file)\n",
    "    skipped_bug_ids = flag_df[flag_df['run_flag'] == 0]['bug_id'].tolist()\n",
    "    for i, bug_id in enumerate(skipped_bug_ids):\n",
    "        skipped_bug_ids[i] = str(bug_id)\n",
    "    skipped_bug_ids = set(skipped_bug_ids)\n",
    "    \n",
    "    for i, row in original_df.iterrows():\n",
    "        bug_id = int(row['bug_id'])        \n",
    "        \n",
    "        if not os.path.exists(f'../data/keywords/{project}/{gpt_variant}/run_{run}/{bug_id}.txt'):\n",
    "            summaries.append(original_df.loc[i, 'short_desc'])\n",
    "            descriptions.append(original_df.loc[i, 'description'])\n",
    "            bug_ids.append(bug_id)\n",
    "            continue\n",
    "        \n",
    "        if str(bug_id) in skipped_bug_ids:\n",
    "            summaries.append(original_df.loc[i, 'short_desc'])\n",
    "            descriptions.append(original_df.loc[i, 'description'])\n",
    "            bug_ids.append(bug_id)\n",
    "            continue\n",
    "        \n",
    "        with open(f'../data/keywords/{project}/{gpt_variant}/run_{run}/{bug_id}.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        all_bugs.add(bug_id)            \n",
    "        rephrased_summary, rephrased_description = '', ''\n",
    "        found_response = False\n",
    "        found_summary_description = False\n",
    "        response_begin = -1\n",
    "        \n",
    "        for line_index, line in zip(range(len(lines)), lines):\n",
    "            if '>>>>>> Response:' in line:    \n",
    "                found_response = True\n",
    "                response_begin = line_index + 1\n",
    "                continue\n",
    "            \n",
    "            if found_response:\n",
    "                summary_match = re.match(r'^Summary:\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                \n",
    "                if summary_match:\n",
    "                    rephrased_summary = summary_match.group(1)\n",
    "                \n",
    "                if len(rephrased_summary) > 0:\n",
    "                    description_match = re.match(r'^Description:\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                    if description_match:\n",
    "                        rephrased_description = description_match.group(1)\n",
    "                        summaries.append(rephrased_summary)\n",
    "                        descriptions.append(rephrased_description)\n",
    "                        bug_ids.append(bug_id)\n",
    "                        found_bugs.add(bug_id)\n",
    "                        rephrased_summary = ''\n",
    "                        rephrased_description = ''\n",
    "                        found_response = False\n",
    "                        modified_bug_count += 1\n",
    "                        found_summary_description = True\n",
    "                        break\n",
    "        \n",
    "        if not found_summary_description and found_response:\n",
    "            # os.remove(f'../data/keywords/{project}/{gpt_variant}/run_{run}/{bug_id}.txt') \n",
    "            # ic(response_begin)        \n",
    "            # print(f'No summary and description found for bug {bug_id}')\n",
    "            # Regular expression to match each line starting with a hyphen\n",
    "            regex = r\"-\\s*(.+)\"\n",
    "            mixed_keywords += 1\n",
    "            for line_idx in range(response_begin, len(lines)):\n",
    "                line = lines[line_idx]\n",
    "                matches = re.findall(regex, line)\n",
    "                if len(matches) > 0:                    \n",
    "                    rephrased_summary += matches[0] + ' '                \n",
    "                    rephrased_description += matches[0] + ' '\n",
    "                                    \n",
    "            # ic(bug_id)\n",
    "            # ic(rephrased_summary, rephrased_description)\n",
    "            bug_ids.append(bug_id)\n",
    "            summaries.append(rephrased_summary)\n",
    "            descriptions.append(rephrased_description)\n",
    "            rephrased_summary = ''\n",
    "            rephrased_description = ''\n",
    "            found_response = False\n",
    "            modified_bug_count += 1\n",
    "            found_bugs.add(bug_id)\n",
    "        \n",
    "        found_response = False\n",
    "        found_summary_description = False\n",
    "        \n",
    "        bug_dict = dict()\n",
    "        for bug in bug_ids:\n",
    "            bug_dict[bug] = bug_dict.get(bug, 0) + 1\n",
    "        for bug in bug_dict:\n",
    "            if bug_dict[bug] == 2:\n",
    "                print(bug)\n",
    "            if bug_dict[bug] == 0:\n",
    "                print(bug)\n",
    "    ic(run)\n",
    "    ic(all_bugs - found_bugs)\n",
    "    ic(len(bug_ids), len(summaries), len(descriptions))\n",
    "    ic(modified_bug_count)\n",
    "    ic(mixed_keywords)\n",
    "    # pd.DataFrame({\n",
    "    #     'bug_id': bug_ids, \n",
    "    #     'short_desc': summaries, \n",
    "    #     'description': descriptions\n",
    "    # }).to_csv(to_save_csv)\n",
    "    \n",
    "project = 'spark'\n",
    "gpt_variant = 'gpt_verbose'\n",
    "\n",
    "for run in range(1, 6):\n",
    "    extract_text_generated_by_gpt(project, \\\n",
    "        f'../data/raw/test_{project}.csv', \\\n",
    "        f'../data/keywords/{project}/{gpt_variant}_test_{run}.csv', \\\n",
    "        run, \\\n",
    "        gpt_variant, \\\n",
    "        f'../data/ablation/test_{project}_flag_content.csv', \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| run: 1\n",
      "ic| all_bugs - found_bugs: set()\n",
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n",
      "ic| modified_bug_count: 1466\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "import re, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_generated_by_llm(project, original_csv, to_save_csv, run, llm):\n",
    "    original_df = pd.read_csv(original_csv)\n",
    "    bug_ids = []\n",
    "    summaries, descriptions = [], []\n",
    "    all_bugs = set()\n",
    "    found_bugs = set()\n",
    "    modified_bug_count = 0\n",
    "        \n",
    "    for i, row in original_df.iterrows():\n",
    "        bug_id = int(row['bug_id'])\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        \n",
    "        if not os.path.exists(f'../data/keywords/{project}/{llm}/run_{run}/{bug_id}.txt'):\n",
    "            summaries.append(original_df.loc[i, 'short_desc'])\n",
    "            descriptions.append(original_df.loc[i, 'description'])\n",
    "            bug_ids.append(bug_id)\n",
    "            continue\n",
    "        \n",
    "        with open(f'../data/keywords/{project}/{llm}/run_{run}/{bug_id}.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        all_bugs.add(bug_id)            \n",
    "        rephrased_summary, rephrased_description = '', ''\n",
    "        found_response = False\n",
    "        found_summary_description = False\n",
    "        response_begin = -1\n",
    "        \n",
    "        for line_index, line in zip(range(len(lines)), lines):\n",
    "            if '>>>>>> Response:' in line:    \n",
    "                found_response = True\n",
    "                response_begin = line_index + 1\n",
    "                continue\n",
    "            \n",
    "            if found_response:\n",
    "                summary_match = re.match(r'^Summary:\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                summary_star_match = re.match(r'^\\*\\*Summary:\\*\\*\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                \n",
    "\n",
    "                if summary_match:\n",
    "                    rephrased_summary = summary_match.group(1)\n",
    "                    # ic(rephrased_summary)\n",
    "                if summary_star_match:\n",
    "                    rephrased_summary = summary_star_match.group(1)\n",
    "                    # ic(rephrased_summary)\n",
    "                    \n",
    "                if len(rephrased_summary) > 0:\n",
    "                    description_match = re.match(r'^Description:\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                    description_star_match = re.match(r'^\\*\\*Description:\\*\\*\\s*(?:\\[)?(.*?)(?:\\])?$', line.strip())\n",
    "                    if description_match or description_star_match:                                               \n",
    "                        if description_match:\n",
    "                            rephrased_description = description_match.group(1)\n",
    "                        elif description_star_match:\n",
    "                            rephrased_description = description_star_match.group(1)\n",
    "                        summaries.append(rephrased_summary)\n",
    "                        descriptions.append(rephrased_description)\n",
    "                        # ic(rephrased_summary, rephrased_description)\n",
    "                        bug_ids.append(bug_id)\n",
    "                        found_bugs.add(bug_id)\n",
    "                        rephrased_summary = ''\n",
    "                        rephrased_description = ''\n",
    "                        found_response = False                        \n",
    "                        modified_bug_count += 1\n",
    "                        found_summary_description = True\n",
    "                        break\n",
    "                    \n",
    "        if not found_summary_description and found_response:\n",
    "            # os.remove(f'../data/keywords/{project}/{llm}/run_{run}/{bug_id}.txt')         \n",
    "            # print(f'No summary and description found for bug {bug_id}')\n",
    "            regex = r\"-\\s*(.+)\"\n",
    "            found_summary_again = False\n",
    "            found_description_again = False\n",
    "            for line_idx in range(response_begin, len(lines)):\n",
    "                line = lines[line_idx]\n",
    "                if 'Summary:' in line:\n",
    "                    found_summary_again = True\n",
    "                    continue\n",
    "                if 'Description:' in line:\n",
    "                    found_description_again = True\n",
    "                    break\n",
    "                    \n",
    "            if found_summary_again and found_description_again:            \n",
    "                description_begin = response_begin + 1\n",
    "                for line_idx in range(response_begin, len(lines)):\n",
    "                    line = lines[line_idx]\n",
    "                    if 'Description:' in line:\n",
    "                        description_begin = line_idx + 1\n",
    "                        break\n",
    "                    matches = re.findall(regex, line)\n",
    "                    if len(matches) > 0:\n",
    "                        rephrased_summary += matches[0] + ' '\n",
    "                \n",
    "                for line_idx in range(description_begin, len(lines)):\n",
    "                    line = lines[line_idx]\n",
    "                    matches = re.findall(regex, line)\n",
    "                    if len(matches) > 0:\n",
    "                        rephrased_description += matches[0] + ' '\n",
    "            else:\n",
    "                for line_idx in range(response_begin, len(lines)):\n",
    "                    line = lines[line_idx]\n",
    "                    matches = re.findall(regex, line)\n",
    "                    if len(matches) > 0:\n",
    "                        rephrased_summary += matches[0] + ' '\n",
    "                        rephrased_description += matches[0] + ' '\n",
    "                                    \n",
    "            # ic(bug_id)\n",
    "            # ic(rephrased_summary, rephrased_description)\n",
    "            bug_ids.append(bug_id)\n",
    "            summaries.append(rephrased_summary)\n",
    "            descriptions.append(rephrased_description)\n",
    "            rephrased_summary = ''\n",
    "            rephrased_description = ''\n",
    "            found_response = False\n",
    "            modified_bug_count += 1\n",
    "            found_bugs.add(bug_id)\n",
    "        \n",
    "        found_response = False\n",
    "        found_summary_description = False\n",
    "                                                               \n",
    "        bug_dict = dict()\n",
    "        for bug in bug_ids:\n",
    "            bug_dict[bug] = bug_dict.get(bug, 0) + 1\n",
    "        for bug in bug_dict:\n",
    "            if bug_dict[bug] == 2:\n",
    "                print(bug)\n",
    "            if bug_dict[bug] == 0:\n",
    "                print(bug)\n",
    "    ic(run)\n",
    "    ic(all_bugs - found_bugs)\n",
    "    ic(len(bug_ids), len(summaries), len(descriptions))\n",
    "    ic(modified_bug_count)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'bug_id': bug_ids, \n",
    "        'short_desc': summaries, \n",
    "        'description': descriptions\n",
    "    }).to_csv(to_save_csv)\n",
    "\n",
    "project = 'spark'\n",
    "llm = 'llama3'\n",
    "\n",
    "for run in range(1, 2):\n",
    "    extract_text_generated_by_llm(project, \\\n",
    "        f'../data/raw/test_{project}.csv', \\\n",
    "        f'../data/keywords/{project}/{llm}_test_{run}.csv', \\\n",
    "        run, \\\n",
    "        llm\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(test_data[1].split()): 2841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 98161.33it/s]\n",
      "ic| project: 'spark'\n",
      "ic| p25: 29.0, p50: 61.0, p75: 125.0\n",
      "ic| len(test_data[1].split()): 3740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 66900.98it/s]\n",
      "ic| project: 'hadoop'\n",
      "ic| p25: 24.0, p50: 48.0, p75: 94.0\n",
      "ic| len(test_data[1].split()): 7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17016/17016 [00:00<00:00, 87954.98it/s]\n",
      "ic| project: 'kibana'\n",
      "ic| p25: 41.0, p50: 74.0, p75: 132.0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def investigate_data(project):\n",
    "    desc_lens = []\n",
    "    \n",
    "    with open('../data/raw/test_{}.txt'.format(project), 'r') as f:\n",
    "        test_data = f.readlines()\n",
    "    ic(len(test_data[1].split()))\n",
    "    test_bug_ids = set()\n",
    "    \n",
    "    for bug_id in test_data[1].split():\n",
    "        test_bug_ids.add(bug_id)\n",
    "    print(len(test_bug_ids))\n",
    "    \n",
    "    original_content_json = '../data/raw/{}.json'.format(project)\n",
    "    with open(original_content_json, 'r') as f:\n",
    "        original_content_lines = f.readlines()\n",
    "    descriptions = []\n",
    "    \n",
    "    for line in tqdm(original_content_lines):\n",
    "        bug = json.loads(line)        \n",
    "        if bug['bug_id'] in test_bug_ids:\n",
    "            continue\n",
    "        \n",
    "        descriptions.append(bug['description'])\n",
    "        # ic(bug['description'])\n",
    "        # ic(bug['bug_id'], len(bug['description'].split()))\n",
    "        desc_lens.append(len(bug['description'].split()))\n",
    "    \n",
    "    p25 = np.percentile(desc_lens, 25)\n",
    "    p50 = np.percentile(desc_lens, 50)\n",
    "    p75 = np.percentile(desc_lens, 75)\n",
    "    ic(project)\n",
    "    ic(p25, p50, p75)\n",
    "    # ic(long_count)\n",
    "    # Spark: ic| p25: 29.0, p50: 61.0, p75: 125.0\n",
    "    # Hadoop: ic| p25: 24.0, p50: 48.0, p75: 94.0\n",
    "    # Kibana: ic| p25: 41.0, p50: 74.0, p75: 132.0\n",
    "\n",
    "for project in ['spark', 'hadoop', 'kibana']:\n",
    "    investigate_data(project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "run_flag\n",
      "0    2255\n",
      "1     586\n",
      "Name: count, dtype: int64\n",
      "hadoop\n",
      "run_flag\n",
      "0    2837\n",
      "1     903\n",
      "Name: count, dtype: int64\n",
      "kibana\n",
      "run_flag\n",
      "0    5284\n",
      "1    1883\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def filter_length(project, length_threshold):\n",
    "    \"\"\"\n",
    "    Ablation study 1: filter out the bug reports whose description \n",
    "    length is less than the threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv('../data/raw/test_{}.csv'.format(project), index_col=None)\n",
    "    bug_ids = []\n",
    "    run_flag = []\n",
    "    for i, row in df.iterrows():\n",
    "        bug_id = row['bug_id']\n",
    "        bug_ids.append(bug_id)\n",
    "        if len(str(row['description']).split()) < length_threshold:\n",
    "            run_flag.append(0)\n",
    "            continue\n",
    "        run_flag.append(1)\n",
    "    df = pd.DataFrame({'bug_id': bug_ids, 'run_flag': run_flag})\n",
    "    print(project)\n",
    "    print(df['run_flag'].value_counts())\n",
    "    df.to_csv('../data/ablation/test_{}_flag_length.csv'.format(project))\n",
    "\n",
    "filter_length('spark', 125)\n",
    "filter_length('hadoop', 94)\n",
    "filter_length('kibana', 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def contains_code_url(s):\n",
    "    patterns = {\n",
    "        \"code_items\": r\"\\w+\\.\\w+\\.\\w{1,}\",\n",
    "        \"urls\": r\"https?://\\S+\"\n",
    "    }\n",
    "    \n",
    "    # Correctly join the values from the patterns dictionary\n",
    "    pattern = re.compile('|'.join(patterns.values()))\n",
    "\n",
    "    # Search for any matches in the string\n",
    "    if pattern.search(s):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def filter_content(project):\n",
    "    \"\"\"\n",
    "    Ablaion study 2: filter out the bug reports whose description does not contain\n",
    "    URLs or code\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('../data/raw/test_{}.csv'.format(project), index_col=None)\n",
    "    bug_ids = []\n",
    "    run_flag = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        bug_id = row['bug_id']\n",
    "        bug_ids.append(bug_id)\n",
    "        if contains_code_url(str(row['description'])):\n",
    "            run_flag.append(1)\n",
    "            continue       \n",
    "        run_flag.append(0)\n",
    "    \n",
    "    df = pd.DataFrame({'bug_id': bug_ids, 'run_flag': run_flag})\n",
    "    print(project)\n",
    "    print(df['run_flag'].value_counts())\n",
    "    df.to_csv('../data/ablation/test_{}_flag_content.csv'.format(project))\n",
    "\n",
    "# for project in ['spark', 'hadoop', 'kibana']:\n",
    "#     filter_content(project)\n",
    "contains_code_url('com.example.MyClass.myMethod(MyClass.java:42)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13320672\n",
      "13313818\n",
      "13347610\n",
      "13315043\n",
      "13316584\n",
      "13316594\n",
      "13284984\n",
      "13285010\n",
      "13285017\n",
      "13294066\n",
      "13294068\n",
      "13278803\n",
      "13328229\n"
     ]
    }
   ],
   "source": [
    "def check_llm_generated_data(project, llm, flag_file):\n",
    "    bug_ids = set()\n",
    "    flag_df = pd.read_csv(flag_file)\n",
    "    for i, row in flag_df.iterrows():\n",
    "        if row['run_flag'] == 1:\n",
    "            bug_ids.add(row['bug_id'])\n",
    "    for bug_id in bug_ids:\n",
    "        if not os.path.exists(f'../data/keywords/{project}/{llm}/run_1/{bug_id}.txt'):\n",
    "            print(bug_id)\n",
    "            continue\n",
    "check_llm_generated_data('spark', 'openchat', '../data/ablation/test_spark_flag_content.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Length Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| df.shape: (2841, 3)\n",
      "ic| df.shape: (3740, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "run_flag\n",
      "1    1583\n",
      "0    1258\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| df.shape: (7167, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "run_flag\n",
      "0    1895\n",
      "1    1845\n",
      "Name: count, dtype: int64\n",
      "kibana\n",
      "run_flag\n",
      "1    3791\n",
      "0    3376\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def filter_length_content(project, length_threshold):\n",
    "    \"\"\"\n",
    "    Ablation study 3: filter the bug reports whose description length is greater than or equal to the threshold\n",
    "    or contains code\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('../data/raw/test_{}.csv'.format(project), index_col=None)\n",
    "    ic(df.shape)\n",
    "    bug_ids = []\n",
    "    run_flag = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        bug_id = row['bug_id']\n",
    "        bug_ids.append(bug_id)\n",
    "        if len(str(row['description']).split()) >= length_threshold:\n",
    "            run_flag.append(1)\n",
    "            continue\n",
    "        if contains_code_url(str(row['description'])):\n",
    "            run_flag.append(1)\n",
    "            continue\n",
    "        run_flag.append(0)\n",
    "    \n",
    "    df = pd.DataFrame({'bug_id': bug_ids, 'run_flag': run_flag})\n",
    "    print(project)\n",
    "    print(df['run_flag'].value_counts())\n",
    "    df.to_csv(f'../data/ablation/test_{project}_flag_length_content.csv')\n",
    "\n",
    "# filter_length('spark', 125)\n",
    "# filter_length('hadoop', 94)\n",
    "# filter_length('kibana', 132)\n",
    "\n",
    "filter_length_content('spark', 125)\n",
    "filter_length_content('hadoop', 93)\n",
    "filter_length_content('kibana', 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(bug_ids): 2841, len(summaries): 2841, len(descriptions): 2841\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "import re, os\n",
    "\n",
    "def extract_text_generated_by_keyword_extractor(project, original_csv, to_save_csv, run, keyword_extractor):\n",
    "    \n",
    "    original_df = pd.read_csv(original_csv)\n",
    "    bug_ids = []\n",
    "    summaries, descriptions = [], []\n",
    "    \n",
    "    for i, row in original_df.iterrows():\n",
    "        \n",
    "        bug_id = row['bug_id']\n",
    "        \n",
    "        if not os.path.exists('../data/keywords/{}/{}/run_{}/{}.txt'.format(project, keyword_extractor, run, bug_id)):\n",
    "            summaries.append(original_df.loc[i, 'short_desc'])\n",
    "            descriptions.append(original_df.loc[i, 'description'])\n",
    "            bug_ids.append(bug_id)\n",
    "            continue\n",
    "            \n",
    "        with open('../data/keywords/{}/{}/run_{}/{}.txt'.format(project, keyword_extractor, run, bug_id), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        rephrased_summary, rephrased_description = '', ''\n",
    "        \n",
    "        for line in lines:\n",
    "            if \"Summary:\" in line:\n",
    "                rephrased_summary = line.split('Summary:')[1].strip()\n",
    "                continue\n",
    "                \n",
    "            if \"Description:\" in line:\n",
    "                rephrased_description = line.split('Description:')[1].strip()\n",
    "                                 \n",
    "                bug_ids.append(bug_id)\n",
    "                summaries.append(rephrased_summary)\n",
    "                descriptions.append(rephrased_description)\n",
    "                # ic(bug_id, rephrased_summary, rephrased_description)\n",
    "                                                             \n",
    "        bug_dict = dict()\n",
    "        for bug in bug_ids:\n",
    "            bug_dict[bug] = bug_dict.get(bug, 0) + 1\n",
    "        for bug in bug_dict:\n",
    "            if bug_dict[bug] == 2:\n",
    "                print(bug)\n",
    "    \n",
    "    ic(len(bug_ids), len(summaries), len(descriptions))\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'bug_id': bug_ids, \n",
    "        'short_desc': summaries, \n",
    "        'description': descriptions\n",
    "    }).to_csv(to_save_csv)\n",
    "    \n",
    "keyword_extractor = 'kpminer-idf'\n",
    "run = 1\n",
    "\n",
    "project = 'spark'\n",
    "extract_text_generated_by_keyword_extractor(\n",
    "    project, \\\n",
    "    '../data/raw/test_{}.csv'.format(project), \\\n",
    "    '../data/keywords/{}/{}_test_{}.csv'.format(project, keyword_extractor, run), \\\n",
    "    run, keyword_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Back filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(test_bug_ids): 2841\n",
      "100%|██████████| 9579/9579 [00:00<00:00, 18695.55it/s]\n",
      "ic| modified_bug_id_count: 1466\n",
      "ic| cannot_produce: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average summary keywords count: 11.0\n",
      "Average description keywords count: 13.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install ujson\n",
    "import ujson\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def write_back_json_filtering(run, flag_file, to_save_json, project, keyword_extractor):\n",
    "    with open(f'../data/raw/test_{project}.txt', 'r') as f:\n",
    "        test_data = f.readlines()\n",
    "    summary_keywords_count = []\n",
    "    run_ids = []\n",
    "    cannot_produce = 0\n",
    "    description_keywords_count = []\n",
    "    test_bug_ids = set()\n",
    "    for bug_id in test_data[1].split():\n",
    "        test_bug_ids.add(bug_id)        \n",
    "    ic(len(test_bug_ids))\n",
    "    \n",
    "    original_content_json = '../data/raw/{}.json'.format(project)\n",
    "    df = pd.read_csv(f'../data/keywords/{project}/{keyword_extractor}_test_{run}.csv')\n",
    "    \n",
    "    flag_df = pd.read_csv(flag_file)\n",
    "    skipped_bug_ids = flag_df[flag_df['run_flag'] == 0]['bug_id'].tolist()\n",
    "    for i, bug_id in enumerate(skipped_bug_ids):\n",
    "        skipped_bug_ids[i] = str(bug_id)\n",
    "    skipped_bug_ids = set(skipped_bug_ids)\n",
    "    \n",
    "    with open(original_content_json, 'r') as f:\n",
    "        original_content_lines = f.readlines()\n",
    "    \n",
    "    output_lines = []\n",
    "    modified_bug_id_count = 0\n",
    "    for line in tqdm(original_content_lines):\n",
    "        bug = ujson.loads(line)\n",
    "        \n",
    "        if bug['bug_id'] in skipped_bug_ids:\n",
    "            output_lines.append(ujson.dumps(bug))\n",
    "            continue\n",
    "        \n",
    "        if bug['bug_id'] in test_bug_ids:\n",
    "            if not os.path.exists(f'../data/keywords/{project}/{keyword_extractor}/run_{run}/{bug[\"bug_id\"]}.txt'):\n",
    "                print(bug['bug_id'])\n",
    "                cannot_produce += 1\n",
    "                continue\n",
    "            try:\n",
    "                row = df[df['bug_id'] == int(bug['bug_id'])].iloc[0]\n",
    "            except IndexError:\n",
    "                print(type(bug['bug_id']))\n",
    "                print(bug['bug_id'])\n",
    "                \n",
    "            bug['short_desc'] = row['short_desc']\n",
    "            summary_keywords_count.append(len(str(row['short_desc']).split()))\n",
    "            bug['description'] = row['description']\n",
    "            description_keywords_count.append(len(str(row['description']).split()))\n",
    "            modified_bug_id_count += 1\n",
    "            run_ids.append(bug['bug_id'])\n",
    "        \n",
    "        output_lines.append(ujson.dumps(bug))\n",
    "    \n",
    "    with open(to_save_json, 'w') as f:\n",
    "        for bug in output_lines:\n",
    "            f.write(bug)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    ic(modified_bug_id_count)\n",
    "    ic(cannot_produce)\n",
    "    print('Average summary keywords count:', round(sum(summary_keywords_count) / len(summary_keywords_count), 1))\n",
    "    print('Average description keywords count:', round(sum(description_keywords_count) / len(description_keywords_count), 1))\n",
    "    for i, desc_len in zip(range(len(description_keywords_count)), description_keywords_count):\n",
    "        if desc_len > 1000:\n",
    "            print(run_ids[i], i, desc_len)\n",
    "            \n",
    "\n",
    "\n",
    "keyword_extractor = 'kpminer-idf'\n",
    "project = 'spark'\n",
    "\n",
    "for run in range(1, 2):\n",
    "    # write_back_json_filtering(run, \\\n",
    "    # f'../data/ablation/test_{project}_flag_length.csv', \\\n",
    "    # f'../data/ablation/{project}_gpt_length_r{run}.json', \\\n",
    "    #     project)\n",
    "    \n",
    "    write_back_json_filtering(run, \\\n",
    "    f'../data/ablation/test_{project}_flag_content.csv', \\\n",
    "    f'../data/ablation/{project}_{keyword_extractor}_content_r{run}.json', \\\n",
    "    project, \\\n",
    "    keyword_extractor)\n",
    "    \n",
    "    # write_back_json_filtering(run, \\\n",
    "    # f'../data/ablation/test_{project}_flag_length_content.csv', \\\n",
    "    # f'../data/ablation/{project}_gpt_length_content_r{run}.json', \\\n",
    "    # project,\n",
    "    # keyword_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ujson\n",
    "\n",
    "def write_back_json(project, flag_file, run, to_save_json):\n",
    "    with open('../data/raw/test_{}.txt'.format(project), 'r') as f:\n",
    "        test_data = f.readlines()        \n",
    "    test_bug_ids = set()\n",
    "    for bug_id in test_data[1].split():\n",
    "        test_bug_ids.add(bug_id)\n",
    "    ic(len(test_bug_ids))\n",
    "    \n",
    "    original_content_json = '../data/raw/{}.json'.format(project)\n",
    "    df = pd.read_csv('../data/keywords/{}/{}_test_{}.csv'.format(project, keyword_extractor, run))\n",
    "    \n",
    "    with open(original_content_json, 'r') as f:\n",
    "        original_content_lines = f.readlines()\n",
    "    \n",
    "    flag_df = pd.read_csv(flag_file)\n",
    "    skipped_bug_ids = flag_df[flag_df['run_flag'] == 0]['bug_id'].tolist()\n",
    "    for i, bug_id in enumerate(skipped_bug_ids):\n",
    "        skipped_bug_ids[i] = str(bug_id)\n",
    "    skipped_bug_ids = set(skipped_bug_ids)\n",
    "    \n",
    "    output_lines = []\n",
    "    for line in tqdm(original_content_lines):\n",
    "        bug = ujson.loads(line)\n",
    "            \n",
    "        if bug['bug_id'] in test_bug_ids:\n",
    "            try:\n",
    "                row = df[df['bug_id'] == int(bug['bug_id'])].iloc[0]\n",
    "            except IndexError:\n",
    "                print(type(bug['bug_id']))\n",
    "                print(bug['bug_id'])\n",
    "            bug['short_desc'] = row['short_desc']\n",
    "            bug['description'] = row['description']\n",
    "        \n",
    "        output_lines.append(ujson.dumps(bug))\n",
    "    \n",
    "    with open(to_save_json, 'w') as f:\n",
    "        for bug in output_lines:\n",
    "            f.write(bug)\n",
    "            f.write('\\n')\n",
    "      \n",
    "keyword_extractor = 'tfidf'\n",
    "project = 'spark'\n",
    "run = 1\n",
    "write_back_json(project, run, '../data/keywords/{}/{}_{}_p1_r{}.json'.format(project, project, keyword_extractor, run))\n",
    "# for run in range(1, 6):\n",
    "    # write_back_json(project, run, '../data/keywords/{}/{}_{}_p1_r{}.json'.format(project, project, keyword_extractor, run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Two Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| total_diff: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import filecmp\n",
    "import os\n",
    "\n",
    "def compare_files(folder1, folder2, filename):\n",
    "    \"\"\"\n",
    "    Check whether two files are the same.\n",
    "    \"\"\"\n",
    "    \n",
    "    file1 = folder1 + '/' + filename\n",
    "    file2 = folder2 + '/' + filename\n",
    "    \n",
    "    if not filecmp.cmp(file1, file2):        \n",
    "    #     print(f\"The files {file1} and {file2} are different.\")\n",
    "        return 1\n",
    "    return 0\n",
    "        \n",
    "# folder1 = '../data/keywords/spark/llama3/run_3'\n",
    "\n",
    "folder1 = '../data/keywords/spark/llama3/run_1'\n",
    "folder2 = '../data/keywords/spark/llama3/run_2'\n",
    "# folder2 = '../data/keywords/spark/llama3-new/run_1'\n",
    "\n",
    "total_diff = 0\n",
    "for file in os.listdir(folder2):\n",
    "    total_diff += compare_files(folder1, folder2, file)\n",
    "ic(total_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(length_flag_1): 31\n",
      "    len(content_flag_1): 62\n",
      "    len(length_content_flag_1): 63\n",
      "ic|"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " length_content_flag_1 - content_flag_1: {13307183}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{13307183}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoop_length_flag = pd.read_csv('../data/ablation/test_hadoop_flag_length.csv')\n",
    "hadoop_content_flag = pd.read_csv('../data/ablation/test_hadoop_flag_content.csv')\n",
    "hadoop_length_content_flag = pd.read_csv('../data/ablation/test_hadoop_flag_length_content.csv')\n",
    "length_flag_1 = set(hadoop_length_flag[hadoop_length_flag['run_flag'] == 1]['bug_id'].tolist())\n",
    "content_flag_1 = set(hadoop_content_flag[hadoop_content_flag['run_flag'] == 1]['bug_id'].tolist())\n",
    "length_content_flag_1 = set(hadoop_length_content_flag[hadoop_length_content_flag['run_flag'] == 1]['bug_id'].tolist())\n",
    "ic(len(length_flag_1), len(content_flag_1), len(length_content_flag_1))\n",
    "ic(length_content_flag_1 - content_flag_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

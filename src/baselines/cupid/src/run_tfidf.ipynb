{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tfidf_extract(text):\n",
    "    extractor = pke.unsupervised.TfIdf()\n",
    "    df_counts = pke.load_document_frequency_file(input_file='../data/spark_df.tsv.gz')\n",
    "    extractor.load_document(input=text, language='en')\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting(df=df_counts)\n",
    "    keyphrases = extractor.get_n_best()\n",
    "    returned_keyphrases = []\n",
    "    for keyphrase in keyphrases:\n",
    "        keyphrase = keyphrase[0]\n",
    "        returned_keyphrases.append(keyphrase)\n",
    "    return ' '.join(returned_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(test_bug_ids): 2841\n",
      "100%|██████████| 9579/9579 [00:01<00:00, 5599.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import ujson, os\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "\n",
    "raw_json = '../data/raw/spark.json'\n",
    "with open('../data/raw/test_spark.txt', 'r') as f:\n",
    "        test_data = f.readlines()        \n",
    "test_bug_ids = set()\n",
    "for bug_id in test_data[1].split():\n",
    "    test_bug_ids.add(bug_id)\n",
    "ic(len(test_bug_ids))\n",
    "\n",
    "compute_df = '../data/compute_df/'\n",
    "if not os.path.exists(compute_df):\n",
    "    os.makedirs(compute_df)\n",
    "    \n",
    "with open(raw_json, 'r') as f:\n",
    "    data = f.readlines()\n",
    "for line in tqdm(data):\n",
    "    bug = ujson.loads(line)\n",
    "    bug_id = bug['bug_id']\n",
    "    if bug_id in test_bug_ids:\n",
    "        continue\n",
    "    if not os.path.exists(f'{compute_df}/{bug_id}_summary.txt'):\n",
    "        with open(f'{compute_df}/{bug_id}_summary.txt', 'w') as f:\n",
    "            f.write(bug['short_desc'])\n",
    "        with open(f'{compute_df}/{bug_id}_description.txt', 'w') as f:\n",
    "            f.write(bug['description'])\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/boudinfl/pke.git\n",
    "\n",
    "from pke import compute_document_frequency\n",
    "from string import punctuation\n",
    "\n",
    "\"\"\"Compute Document Frequency (DF) counts from a collection of documents.\n",
    "\n",
    "N-grams up to 3-grams are extracted and converted to their n-stems forms.\n",
    "Those containing a token that occurs in a stoplist are filtered out.\n",
    "Output file is in compressed (gzip) tab-separated-values format (tsv.gz).\n",
    "\"\"\"\n",
    "\n",
    "# stoplist for filtering n-grams\n",
    "stoplist=list(punctuation)\n",
    "\n",
    "# compute df counts and store as n-stem -> weight values\n",
    "compute_document_frequency(documents=compute_df,\n",
    "                           output_file='../data/spark_df.tsv.gz',\n",
    "                           language='en',                # language of files\n",
    "                           normalization=\"stemming\",    # use porter stemmer\n",
    "                           stoplist=stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [25:13<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "project = 'spark'\n",
    "method = 'tfidf'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "tdidf_folder = f'../data/keywords/{project}/tfidf-idf/run_1/'\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "if not os.path.exists(tdidf_folder):\n",
    "    os.makedirs(tdidf_folder.format(project))\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    bug_id = row['bug_id']\n",
    "    if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "        continue\n",
    "    if os.path.exists(os.path.join(tdidf_folder, f'{bug_id}.txt')):\n",
    "        continue\n",
    "    short_desc = row['short_desc']\n",
    "    description = row['description']\n",
    "    short_desc_keywords = tfidf_extract(short_desc)\n",
    "    description_keywords = tfidf_extract(description)\n",
    "    \n",
    "    with open(tdidf_folder.format(project) + '/{}.txt'.format(bug_id), 'w') as f:\n",
    "        f.write('Summary: {}'.format(short_desc_keywords))\n",
    "        f.write('\\n')\n",
    "        f.write('Description: {}'.format(description_keywords))\n",
    "        f.write('\\n')  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "%env OPENAI_API_KEY=INSERT_API_KEY\n",
    "from openai import OpenAI\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'Provide a concise prompt or template that can be used to identify keywords from the summary and description of a bug report. These keywords will be used as input to detect duplicate bug reports. The output format will be:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords].')\n"
     ]
    }
   ],
   "source": [
    "gpt_assistant_prompt = \"\"\n",
    "gpt_user_prompt = \"Provide a concise prompt or template that can be used to identify keywords from the summary and description of a bug report. These keywords will be used as input to detect duplicate bug reports. The output format will be:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords].\"\n",
    "gpt_prompt = gpt_assistant_prompt, gpt_user_prompt\n",
    "print(gpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\n",
      "\n",
      "Output format:\n",
      "Summary: [Selected Keywords]\n",
      "Description: [Selected Keywords]\n"
     ]
    }
   ],
   "source": [
    "message=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": gpt_user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = message,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    frequency_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise prompt: \n",
      "Identify keywords from the bug report to detect duplicates.\n",
      "\n",
      "Verbose prompt:\n",
      "Review the summary and description of the bug report to identify specific keywords that can be used as criteria for detecting duplicate reports. Consider the language used, technical terms, and any unique identifiers mentioned in the report. \n",
      "\n",
      "Output format:\n",
      "Summary: [Selected Keywords]\n",
      "Description: [Selected Keywords]\n"
     ]
    }
   ],
   "source": [
    "alternative_prompt_template = \"Provide two alternative prompts: one is more concise, and the other is more verbose. The current prompt template is:\\n\\nIdentify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "client = OpenAI()\n",
    "message=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": alternative_prompt_template\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = message,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    frequency_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [00:00<00:00, 3469.50it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3444.36it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3492.89it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3476.02it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3473.62it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "for project in ['spark']:\n",
    "    df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "    flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_length_content.csv')\n",
    "\n",
    "    gpt_folder = '../data/keywords/{}/gpt/run_{}'\n",
    "    \n",
    "    for run in range(1, 6):\n",
    "        client = OpenAI()\n",
    "        \n",
    "        if not os.path.exists(gpt_folder.format(project, run)):\n",
    "            os.makedirs(gpt_folder.format(project, run))\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            bug_id = row['bug_id']\n",
    "            \n",
    "            if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "                continue\n",
    "            \n",
    "            message=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt_template.format(row['short_desc'], row['description'])\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages = message,\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    seed=42\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                message = [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt_template.format(row['short_desc'], row['description'][:2000])\n",
    "                    }\n",
    "                ]\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages = message,\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    seed=42\n",
    "                )\n",
    "            \n",
    "            with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "                f.write(prompt_template.format(row['short_desc'], row['description']))\n",
    "                f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "                f.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [00:13<00:00, 203.06it/s]\n",
      "100%|██████████| 2841/2841 [00:09<00:00, 314.01it/s]\n",
      "100%|██████████| 2841/2841 [00:10<00:00, 263.45it/s]\n",
      "100%|██████████| 2841/2841 [00:11<00:00, 250.04it/s]\n",
      "100%|██████████| 2841/2841 [00:14<00:00, 194.68it/s]\n"
     ]
    }
   ],
   "source": [
    "concise_prompt = \"Identify keywords from the bug report to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "gpt_folder = '../data/keywords/{}/gpt_concise/run_{}'\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(gpt_folder.format(project, run)):\n",
    "        os.makedirs(gpt_folder.format(project, run))\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        \n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "            if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "                os.remove(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'))\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "        \n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": concise_prompt.format(row['short_desc'], row['description'])\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            message = [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": concise_prompt.format(row['short_desc'], row['description'][:2000])\n",
    "                }\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        \n",
    "        with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(concise_prompt.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_prompt = \"Review the summary and description of the bug report to identify specific keywords that can be used as criteria for detecting duplicate reports. Consider the language used, technical terms, and any unique identifiers mentioned in the report.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "gpt_folder = '../data/keywords/{}/gpt_verbose/run_{}'\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "client = OpenAI()\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(gpt_folder.format(project, run)):\n",
    "        os.makedirs(gpt_folder.format(project, run))\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "                continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "        \n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": verbose_prompt.format(row['short_desc'], row['description'])\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            message = [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": verbose_prompt.format(row['short_desc'], row['description'][:2000])\n",
    "                }\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        \n",
    "        with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(verbose_prompt.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17016/17016 [00:00<00:00, 135201.71it/s]\n"
     ]
    }
   ],
   "source": [
    "project = 'kibana'\n",
    "\n",
    "with open('../data/raw/test_{}.txt'.format(project), 'r') as f:\n",
    "        test_data = f.readlines()\n",
    "        \n",
    "test_bug_ids = set()\n",
    "for bug_id in test_data[1].split():\n",
    "    test_bug_ids.add(bug_id)\n",
    "print(len(test_bug_ids))\n",
    "\n",
    "original_content_json = '../data/raw/{}.json'.format(project)\n",
    "\n",
    "with open(original_content_json, 'r') as f:\n",
    "    original_content_lines = f.readlines()\n",
    "\n",
    "bug_ids, short_descs, descriptions = [], [], []\n",
    "for line in tqdm(original_content_lines):\n",
    "    bug = json.loads(line)        \n",
    "    if bug['bug_id'] in test_bug_ids:\n",
    "        bug_ids.append(bug['bug_id'])\n",
    "        short_descs.append(bug['short_desc'])\n",
    "        descriptions.append(bug['description'])\n",
    "        \n",
    "test_data = pd.DataFrame({'bug_id': bug_ids, 'short_desc': short_descs, 'description': descriptions})\n",
    "test_data.to_csv('../data/raw/test_{}.csv'.format(project), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBc2CtGYZTCO",
    "outputId": "7f88a78c-0e6e-447b-c7ba-1889c815e086"
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from scipy.stats import skew\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77GiYleYVlax"
   },
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "a82T-YTgU1nb",
    "outputId": "dded758c-3bff-4679-f670-aaad6a2c2563"
   },
   "outputs": [],
   "source": [
    "#importing the CSV file of the dataset from google drive into a dataframe\n",
    "train_eclipse_df=pd.read_csv('')\n",
    "train_eclipse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "FKNb9p7tVkhS",
    "outputId": "9b687134-037a-4bda-86da-a41299d98665"
   },
   "outputs": [],
   "source": [
    "#importing the CSV file of the dataset from google drive into a dataframe\n",
    "test_eclipse_df=pd.read_csv('')\n",
    "test_eclipse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Q9mDbnx-YOhp",
    "outputId": "f9e94c6d-d580-406f-d611-706543d3e42e"
   },
   "outputs": [],
   "source": [
    "#Merge train & test\n",
    "frames = [train_eclipse_df, test_eclipse_df]\n",
    "result = pd.concat(frames)\n",
    "result = result.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "result1 = result.copy()\n",
    "dup_df= result.copy()\n",
    "dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "iUe8hMYBY31i",
    "outputId": "66ab34bb-d319-4443-e4f2-38781c8a60c4"
   },
   "outputs": [],
   "source": [
    "FEATURES = ['description1', 'description2']\n",
    "print('Description of length of the feature columns')\n",
    "dup_df[FEATURES].apply(lambda col: col.str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAV0wX2jxSWJ"
   },
   "source": [
    "## Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydZiq_CqxTVw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "xAlQ7mFiVvax",
    "outputId": "e2d02432-f344-45d6-b6b8-73c396883d89"
   },
   "outputs": [],
   "source": [
    "train_val_df = train_eclipse_df.copy()\n",
    "train_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "FjyZNriwxU8E",
    "outputId": "92e65c09-fb2a-4964-e146-a58ab1691fef"
   },
   "outputs": [],
   "source": [
    " test_df = test_eclipse_df.copy()\n",
    " test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4RGc4NYxX3S"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "  train_val_df,\n",
    "  test_size=0.2,\n",
    "  stratify=train_val_df.is_similar,\n",
    "  random_state=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRlsFfTExl6w",
    "outputId": "6fadea05-0834-4f2e-eb90-196d375985e6"
   },
   "outputs": [],
   "source": [
    "print(f'Train Val Test Size: {len(train_df):,} {len(val_df):,} {len(test_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWklXbxt0AfP"
   },
   "source": [
    "## Download & Prepare Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KE3Hyyi1BhD"
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gm-gZy60BSn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.utils as kutils\n",
    "# from keras.layers.preprocessing.text_vectorization import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9JzrVk29BPW",
    "outputId": "7143df89-54fb-4ecf-c7c8-171e909b6022"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2oZQzY-U_7h",
    "outputId": "9369bb52-f533-4f03-9230-ba1888b62a78"
   },
   "outputs": [],
   "source": [
    "pip install kutils==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMXVuWzb9DvD"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb5Nve7T9Ijz"
   },
   "outputs": [],
   "source": [
    "glove_file = datapath('glove.6B.300d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dIa-73e8JpE",
    "outputId": "a8bd02f8-0b76-40d7-962e-7bc516c303b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23Kxa6pf8PTT"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'embeddings_index' : embeddings_index } , open('glove.6B.300d.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbGW6MLl8RB8"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import itertools\n",
    "from typing import List, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0twgBVfbrJeH"
   },
   "source": [
    "## create vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cItkSImjrL-p"
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS = 20000\n",
    "MAX_TITLE_LENGTH = 21\n",
    "MAX_DESCRIPTION_LENGTH = 500\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQqko955rNdW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def build_vocab(sentences: List[str], sequence_length: int) -> Tuple[TextVectorization, Dict[str, int]]:\n",
    "    # Initialize the TextVectorization layer\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=MAX_TOKENS - 2,\n",
    "        output_sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "    # Fit the vectorizer on the text data\n",
    "    vectorizer.adapt(sentences)\n",
    "\n",
    "    # Get the vocabulary\n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "    # Create a word index dictionary\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    return vectorizer, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "FPWIJxkprpq3",
    "outputId": "a9bfb36d-0d3f-4740-eb31-0dc4608f8388"
   },
   "outputs": [],
   "source": [
    "dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2omBaiRI4I7t"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlWPcs0ArPg-",
    "outputId": "fd36cb02-d87c-4374-c051-117dee77232c"
   },
   "outputs": [],
   "source": [
    "# Flatten the lists of descriptions\n",
    "all_descriptions = dup_df['description1'].tolist() + dup_df['description2'].tolist()\n",
    "\n",
    "# Build the vocabulary\n",
    "descr_vectorizer, descr_word_index = build_vocab(all_descriptions, MAX_DESCRIPTION_LENGTH)\n",
    "\n",
    "# Print the most frequent description words\n",
    "import itertools\n",
    "print(\n",
    "    'Most frequent description words:',\n",
    "    list(itertools.islice(descr_word_index.keys(), 5))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0EDNUlKsDiP"
   },
   "source": [
    "## Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_ZZge2ysE2r"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(\n",
    "  embeddings_index: Dict[str, np.ndarray],\n",
    "  word_index: Dict[str, int],\n",
    "  verbose=False,\n",
    "):\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "\n",
    "  # prepare embedding matrix\n",
    "  embedding_matrix = np.zeros((MAX_TOKENS, EMBEDDING_DIM))\n",
    "  for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # Words not found in embedding index will be all-zeros.\n",
    "      # This includes the representation for \"padding\" and \"OOV\"\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "      hits += 1\n",
    "    else:\n",
    "      misses += 1\n",
    "\n",
    "  if verbose:\n",
    "    print('Embedding shape:', embedding_matrix.shape)\n",
    "    print(f'Found {hits} words, missed {misses}.')\n",
    "\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmnH53lhYfur",
    "outputId": "b9fa4450-20ab-4d3e-f90c-89199bf41579"
   },
   "outputs": [],
   "source": [
    "print('Creating description embedding matrix:')\n",
    "descr_embedding_matrix = create_embedding_matrix(\n",
    "    embeddings_index, descr_word_index, True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiVmA9BquIfW"
   },
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0Rfq90VuJSt"
   },
   "outputs": [],
   "source": [
    "def generate_batches(\n",
    "    split_df: pd.DataFrame,\n",
    "    batch_size=512,\n",
    "):\n",
    "    steps_per_epoch = len(split_df) // batch_size\n",
    "    while True:\n",
    "        for i in range(steps_per_epoch):\n",
    "            offset = i * batch_size\n",
    "            till = offset + batch_size\n",
    "            feature_batches = []\n",
    "\n",
    "            for feature in FEATURES:\n",
    "                vectorizer = title_vectorizer if feature.startswith('title') else descr_vectorizer\n",
    "                # Apply the vectorizer and ensure the result is converted to a Tensor\n",
    "                feature_batch = vectorizer(\n",
    "                    split_df[feature][offset:till].to_numpy().reshape((-1, 1))\n",
    "                ).numpy()\n",
    "                feature_batches.append(tf.convert_to_tensor(feature_batch))\n",
    "\n",
    "            # Convert the target batch to a TensorFlow tensor\n",
    "            target_batch = tf.convert_to_tensor(split_df.is_similar[offset:till].to_numpy(), dtype=tf.float32)\n",
    "\n",
    "            # Yield a tuple of feature batches (as Tensors) and the target batch\n",
    "            yield (tuple(feature_batches), target_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RmhmXHOuOum"
   },
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cfVI21muPqw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD1j962RuSTs"
   },
   "outputs": [],
   "source": [
    "DescrEmbeddingLayer = layers.Embedding(\n",
    "  input_dim=MAX_TOKENS,\n",
    "  output_dim=EMBEDDING_DIM,\n",
    "  embeddings_initializer=Constant(descr_embedding_matrix),\n",
    "  trainable=False,\n",
    "  name='DescrEmbeddingLayer',\n",
    ")\n",
    "\n",
    "def make_descr_layer(num, kernel_size=3, pool_size=2, strides=None):\n",
    "  DescrConv1dLayer = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=kernel_size,\n",
    "    activation='relu',\n",
    "    name=f'DescrConv1dLayer{num}',\n",
    "  )\n",
    "  DescrMaxPool1dLayer = layers.MaxPool1D(\n",
    "    pool_size=pool_size,\n",
    "    strides=strides,\n",
    "    name=f'DescrMaxPool1dLayer{num}',\n",
    "  )\n",
    "  return DescrConv1dLayer, DescrMaxPool1dLayer\n",
    "\n",
    "DescrConv1dLayer1, DescrMaxPool1dLayer1 = make_descr_layer(1, pool_size=4)\n",
    "DescrConv1dLayer2, DescrMaxPool1dLayer2 = make_descr_layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hKaKSfpwuWrU",
    "outputId": "5df934c8-b25e-4442-edb4-6ce1257bc4d2"
   },
   "outputs": [],
   "source": [
    "def create_siamese_component(num: int):\n",
    "  class SiameseComponent:\n",
    "    def __init__(self, title: layers.Input, description: layers.Input, output: layers.Concatenate):\n",
    "      self.title = title\n",
    "      self.description = description\n",
    "      self.output = output\n",
    "\n",
    "  title_input = layers.Input(shape=(None,), dtype='int32', name=f'title{num}_input')\n",
    "\n",
    "  descr_input = layers.Input(shape=(None,), name=f'descr{num}_input')\n",
    "  descr_embedding_layer = DescrEmbeddingLayer(descr_input)\n",
    "  descr_conv1d1 = DescrConv1dLayer1(descr_embedding_layer)\n",
    "  descr_max_pool1d1 = DescrMaxPool1dLayer1(descr_conv1d1)\n",
    "  descr_conv1d2 = DescrConv1dLayer2(descr_max_pool1d1)\n",
    "  descr_max_pool1d2 = DescrMaxPool1dLayer2(descr_conv1d2)\n",
    "  # descr_conv1d3 = DescrConv1dLayer3(descr_max_pool1d2)\n",
    "  # descr_max_pool1d3 = DescrMaxPool1dLayer3(descr_conv1d3)\n",
    "  descr_flat_Layer = layers.Flatten(name=f'FlatDescr{num}')(descr_max_pool1d2)\n",
    "\n",
    "  concat = layers.Concatenate(axis=1, name=f'Concat{num}')([descr_flat_Layer])\n",
    "  return SiameseComponent(title_input, descr_input, concat)\n",
    "\n",
    "\n",
    "component1 = create_siamese_component(1)\n",
    "component2 = create_siamese_component(2)\n",
    "\n",
    "dot_product_layer = layers.Dot(\n",
    "  axes=1,\n",
    "  name='dot_product_layer'\n",
    ")([component1.output, component2.output])\n",
    "output = layers.Dense(\n",
    "  1, activation='sigmoid', name='output',\n",
    ")(dot_product_layer)\n",
    "siamese_model = models.Model(\n",
    "  inputs=[component1.description, component2.description],\n",
    "  outputs=output,\n",
    "  name='siamese_model'\n",
    ")\n",
    "\n",
    "kutils.plot_model(siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrWZRqkJwBtt"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBJFNAONwD_X"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "siamese_model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['acc'],\n",
    ")\n",
    "callback = callbacks.ModelCheckpoint(\n",
    "    filepath=f'Siamese'\n",
    "             '.epoch-{epoch:02d}-loss-{val_loss:.3f}.keras',\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5omz3YiwJIs",
    "outputId": "90ec6a5f-4305-4225-cd43-81a470a85796"
   },
   "outputs": [],
   "source": [
    "history = siamese_model.fit(\n",
    "  generate_batches(train_df, BATCH_SIZE),\n",
    "  steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
    "  epochs=12,\n",
    "  validation_data=generate_batches(val_df, BATCH_SIZE),\n",
    "  validation_steps=len(val_df) // BATCH_SIZE,\n",
    "  verbose=1,\n",
    "  callbacks= [callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "UK6kHy_0xrN9",
    "outputId": "fc5529eb-9a17-4da9-8cd4-4f43e143b4d8"
   },
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sRgywWh4f6X"
   },
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "plt.plot(history.history['acc'], label='train acc')\n",
    "plt.plot(history.history['val_acc'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('AccVal_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxivE_4KF-lC"
   },
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ_Vlc2U1Sf_",
    "outputId": "fd3cf097-740c-4954-e7f9-0cce6018f970"
   },
   "outputs": [],
   "source": [
    "siamese_model.evaluate(\n",
    "  generate_batches(test_df, BATCH_SIZE),\n",
    "  steps=len(test_df) // BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "D_S1YT7gGC1A",
    "outputId": "a8b73b89-16e4-4c08-844a-231870414286"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = len(test_df)\n",
    "num_batched_instances = len(test_df) - (len(test_df) % BATCH_SIZE)\n",
    "pred_y = siamese_model.predict(\n",
    "  generate_batches(test_df, BATCH_SIZE),\n",
    "  steps=len(test_df) // BATCH_SIZE,\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5esiuBvyHOaZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(\n",
    "  test_df.is_similar[:num_batched_instances],\n",
    "  pred_y > .5,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8OK-tLz5szV"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivdeHQwIIQPp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "value_names = ['TPR', 'FPR', 'Threshold']\n",
    "roc = dict(zip(value_names, roc_curve(\n",
    "  test_df.is_similar[:num_batched_instances],\n",
    "  pred_y,\n",
    ")))\n",
    "pd.DataFrame(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGbIeSFWITbw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(\n",
    "  test_df.is_similar[:num_batched_instances],\n",
    "  pred_y,\n",
    ")\n",
    "\n",
    "print('AUROC score:', auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKRDSyhv-RB8"
   },
   "outputs": [],
   "source": [
    "print('For probability:')\n",
    "print(f'Mean: {pred_y.mean()}, STD: {pred_y.std()}')\n",
    "print('For categorical:')\n",
    "print(f'Mean: {(pred_y > .5).mean()}, STD: {(pred_y > .5).std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i8FGP06ZtbN"
   },
   "source": [
    "## Evaluate on textually similar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctv5i4uwZwQB"
   },
   "outputs": [],
   "source": [
    "#importing the CSV file of the dataset from google drive into a dataframe\n",
    "sim_df=pd.read_csv('')\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weGMLu0eaM4Q"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = len(sim_df)\n",
    "num_batched_instances = len(sim_df) - (len(sim_df) % BATCH_SIZE)\n",
    "pred_y_sim = siamese_model.predict(\n",
    "  generate_batches(sim_df, BATCH_SIZE),\n",
    "  steps=len(sim_df) // BATCH_SIZE,\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oe8168B8ztDW"
   },
   "outputs": [],
   "source": [
    "siamese_model.evaluate(\n",
    "  generate_batches(sim_df, BATCH_SIZE),\n",
    "  steps=len(sim_df) // BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apm5qUZtaTCO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "  sim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_sim > .5,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4UOiMGvaXq_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "value_names = ['TPR', 'FPR', 'Threshold']\n",
    "roc = dict(zip(value_names, roc_curve(\n",
    "  sim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_sim,\n",
    ")))\n",
    "pd.DataFrame(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uv4ot3cnaZy-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(\n",
    "  sim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_sim,\n",
    ")\n",
    "\n",
    "print('AUROC score:', auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6I1HvSh-lfd"
   },
   "outputs": [],
   "source": [
    "print('For probability:')\n",
    "print(f'Mean: {pred_y_sim.mean()}, STD: {pred_y_sim.std()}')\n",
    "print('For categorical:')\n",
    "print(f'Mean: {(pred_y_sim > .5).mean()}, STD: {(pred_y_sim > .5).std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmRmorjEfQb8"
   },
   "source": [
    "## Evaluate on textually dissimilar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5np3yqwfV0V"
   },
   "outputs": [],
   "source": [
    "#importing the CSV file of the dataset from google drive into a dataframe\n",
    "dissim_df=pd.read_csv('')\n",
    "dissim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1-3kMw_fh-9"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = len(dissim_df)\n",
    "num_batched_instances = len(dissim_df) - (len(dissim_df) % BATCH_SIZE)\n",
    "pred_y_dis = siamese_model.predict(\n",
    "  generate_batches(dissim_df, BATCH_SIZE),\n",
    "  steps=len(dissim_df) // BATCH_SIZE,\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4_X1A4gzwD2"
   },
   "outputs": [],
   "source": [
    "siamese_model.evaluate(\n",
    "  generate_batches(dissim_df, BATCH_SIZE),\n",
    "  steps=len(dissim_df) // BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xqhsl_Byfm6N"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "  dissim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_dis > .5,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC2L7lDofqqU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "value_names = ['TPR', 'FPR', 'Threshold']\n",
    "roc = dict(zip(value_names, roc_curve(\n",
    "  dissim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_dis,\n",
    ")))\n",
    "pd.DataFrame(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keUtQxHbfv2M"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auroc = roc_auc_score(\n",
    "  dissim_df.is_similar[:num_batched_instances],\n",
    "  pred_y_dis,\n",
    ")\n",
    "\n",
    "print('AUROC score:', auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhSh7qzv0aT2"
   },
   "outputs": [],
   "source": [
    "print('For probability:')\n",
    "print(f'Mean: {pred_y_dis.mean()}, STD: {pred_y_dis.std()}')\n",
    "print('For categorical:')\n",
    "print(f'Mean: {(pred_y_dis > .5).mean()}, STD: {(pred_y_dis > .5).std()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7i8FGP06ZtbN"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
